# Teoria do LaboratoriÃ³w 2 i 3 - Sieci Neuronowe
## Przewodnik z wyjaÅ›nieniami

## Spis treÅ›ci
1. [Laboratorium 2 - Regresja Logistyczna](#laboratorium-2---regresja-logistyczna)
2. [Laboratorium 3 - Wielowarstwowa SieÄ‡ Neuronowa](#laboratorium-3---wielowarstwowa-sieÄ‡-neuronowa)

---

## Laboratorium 2 - Regresja Logistyczna

### 1. Co to jest regresja logistyczna i po co nam to?

WyobraÅº sobie, Å¼e masz dane pacjenta (wiek, ciÅ›nienie, cholesterol itp.) i chcesz przewidzieÄ‡: czy ten czÅ‚owiek jest chory na serce, czy nie? To jest **klasyczny problem klasyfikacji binarnej** - odpowiedÅº to TAK (1) lub NIE (0).

Regresja logistyczna to najprostszy model, ktÃ³ry to robi. DziaÅ‚a jak prosta "bramka decyzyjna" - bierze wszystkie cechy pacjenta, Å‚Ä…czy je razem i mÃ³wi: "Jest 73% szans, Å¼e ta osoba jest chora". 

**Dlaczego "logistyczna"?** Bo uÅ¼ywa funkcji logistycznej (sigmoid), ktÃ³ra zamienia dowolnÄ… liczbÄ™ na prawdopodobieÅ„stwo.

**Dlaczego zaczynamy od tego?** Bo to najprostszy "neuron" - podstawowy budulec wiÄ™kszych sieci. JeÅ›li zrozumiesz to, zrozumiesz caÅ‚Ä… resztÄ™!

### 2. Funkcja sigmoid - "przeksztaÅ‚cacz na prawdopodobieÅ„stwa"

**Problem:** Model liczy nam jakÄ…Å› wartoÅ›Ä‡, powiedzmy -3.7 lub 12.4. Ale my chcemy prawdopodobieÅ„stwa (coÅ› miÄ™dzy 0 a 1)!

**RozwiÄ…zanie:** Funkcja sigmoid! To matematyczna "zjeÅ¼dÅ¼alnia" ktÃ³ra:
- DuÅ¼e liczby dodatnie (np. 10) â†’ przeksztaÅ‚ca na ~1 (prawie pewne)
- DuÅ¼e liczby ujemne (np. -10) â†’ przeksztaÅ‚ca na ~0 (prawie niemoÅ¼liwe)  
- Zero â†’ przeksztaÅ‚ca na dokÅ‚adnie 0.5 (totalny rzut monetÄ…)

**WzÃ³r:**
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

**Intuicja wizualna:** WyobraÅº sobie gÅ‚adkÄ… literÄ™ "S" leÅ¼Ä…cÄ… na boku. Gdy idziesz w prawo (wiÄ™ksze z), zbliÅ¼asz siÄ™ do 1. Gdy w lewo (mniejsze z), spadasz do 0.

**Pochodna (dlaczego jest waÅ¼na?):**
$$\sigma'(z) = \sigma(z) \cdot (1 - \sigma(z))$$

Ta pochodna jest SUPER wygodna w obliczeniach! JeÅ›li juÅ¼ obliczyÅ‚eÅ› sigmoid(z), to jego pochodna to po prostu wynik pomnoÅ¼ony przez (1 - wynik). Eleganckie!

#### ğŸ”¬ SkÄ…d siÄ™ bierze ten wzÃ³r na pochodnÄ…?

**Wyprowadzenie:**
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

Zapiszmy to jako: $\sigma(z) = (1 + e^{-z})^{-1}$

UÅ¼ywamy **reguÅ‚y Å‚aÅ„cuchowej**: jeÅ›li $f(z) = [g(z)]^n$, to $f'(z) = n[g(z)]^{n-1} \cdot g'(z)$

$$\sigma'(z) = -1 \cdot (1 + e^{-z})^{-2} \cdot \frac{d}{dz}(1 + e^{-z})$$

Pochodna wykÅ‚adniczej: $\frac{d}{dz}(e^{-z}) = -e^{-z}$

$$\sigma'(z) = -1 \cdot (1 + e^{-z})^{-2} \cdot (-e^{-z}) = \frac{e^{-z}}{(1 + e^{-z})^2}$$

Teraz sprytna sztuczka! Rozbijmy $(1 + e^{-z})^2$ w mianowniku:

$$\sigma'(z) = \frac{e^{-z}}{(1 + e^{-z})^2} = \frac{1}{1 + e^{-z}} \cdot \frac{e^{-z}}{1 + e^{-z}}$$

ZauwaÅ¼, Å¼e:
- $\frac{1}{1 + e^{-z}} = \sigma(z)$ 
- $\frac{e^{-z}}{1 + e^{-z}} = \frac{e^{-z} + 1 - 1}{1 + e^{-z}} = 1 - \frac{1}{1 + e^{-z}} = 1 - \sigma(z)$

Zatem:
$$\boxed{\sigma'(z) = \sigma(z) \cdot (1 - \sigma(z))}$$

**PiÄ™kno tego wzoru:** Nie musisz liczyÄ‡ wykÅ‚adniczych od nowa - uÅ¼ywasz juÅ¼ obliczonej wartoÅ›ci sigmoid!

### 3. Jak dziaÅ‚a model? (od Å›rodka)

Model to w zasadzie prosta formuÅ‚a w dwÃ³ch krokach:

**Krok 1: WaÅ¼ona suma**
WeÅº wszystkie cechy pacjenta (np. wiek=50, ciÅ›nienie=140, cholesterol=200), przemnÃ³Å¼ kaÅ¼dÄ… przez "wagÄ™" i dodaj wszystko razem:
$$z = w_1 \cdot wiek + w_2 \cdot ciÅ›nienie + w_3 \cdot cholesterol + b$$

MoÅ¼na to zapisaÄ‡ krÃ³cej jako: $z = w^T x + b$

**Co to sÄ… wagi?** To "waÅ¼noÅ›ci" - jeÅ›li waga jest duÅ¼a, ta cecha ma duÅ¼y wpÅ‚yw. Ujemna waga = cecha obniÅ¼a ryzyko.

**Co to jest bias (b)?** To "punkt startowy" - ogÃ³lne przesuniÄ™cie caÅ‚ego modelu.

**Krok 2: PrzeksztaÅ‚cenie na prawdopodobieÅ„stwo**
$$p(chory) = \sigma(z) = \frac{1}{1 + e^{-z}}$$

Teraz masz liczbÄ™ od 0 do 1 - gotowe prawdopodobieÅ„stwo!


### 4. Funkcja kosztu - jak mierzymy, czy model jest dobry?

**Problem:** Mamy model, ktÃ³ry daje jakieÅ› przewidywania. Ale jak sprawdziÄ‡, czy sÄ… dobre?

Potrzebujemy "karnej" funkcji, ktÃ³ra powie: "Hej, Åºle przewidziaÅ‚eÅ›, dostajesz wysokÄ… karÄ™!" albo "Dobra robota, niska kara!".

#### Binary Cross-Entropy (BCE) - funkcja straty

To jest nasza "kara" za zÅ‚e przewidywania. DziaÅ‚a sprytnie:

**Dla jednej prÃ³bki:**
$$L(y, \hat{y}) = -[y \log(\hat{y}) + (1-y) \log(1-\hat{y})]$$

**Jak to czytaÄ‡?**
- JeÅ›li prawdziwa etykieta $y = 1$ (pacjent chory):
  - Liczy siÄ™ tylko czÄ™Å›Ä‡: $-\log(\hat{y})$
  - JeÅ›li przewidziaÅ‚eÅ› $\hat{y} = 0.9$ (wysoka pewnoÅ›Ä‡) â†’ kara maÅ‚a âœ“
  - JeÅ›li przewidziaÅ‚eÅ› $\hat{y} = 0.1$ (niska pewnoÅ›Ä‡) â†’ kara DUÅ»A âœ—
  
- JeÅ›li prawdziwa etykieta $y = 0$ (pacjent zdrowy):
  - Liczy siÄ™ tylko czÄ™Å›Ä‡: $-\log(1-\hat{y})$
  - JeÅ›li przewidziaÅ‚eÅ› $\hat{y} = 0.1$ (pewnoÅ›Ä‡ choroby niska) â†’ kara maÅ‚a âœ“
  - JeÅ›li przewidziaÅ‚eÅ› $\hat{y} = 0.9$ (pewnoÅ›Ä‡ choroby wysoka) â†’ kara DUÅ»A âœ—

**Dla caÅ‚ego zbioru danych:**
$$J(w, b) = \frac{1}{m} \sum_{i=1}^{m} L(y^{(i)}, \hat{y}^{(i)})$$

Po prostu uÅ›redniamy kary ze wszystkich prÃ³bek!

**Sztuczka programistyczna:** W kodzie dodajemy mikroskopijne `epsilon` (np. $10^{-15}$) do logarytmu, Å¼eby uniknÄ…Ä‡ `log(0)` = `-infinity` i nie zepsuÄ‡ obliczeÅ„.

#### ğŸ”¬ SkÄ…d siÄ™ bierze wzÃ³r na BCE?

**Intuicja z teorii informacji:**

Binary Cross-Entropy pochodzi z **teorii informacji** i mierzy "zaskoczenie" modelu.

**Dla jednej prÃ³bki:**

1. JeÅ›li prawdziwa etykieta to $y=1$ (pacjent chory):
   - Model przewiduje $\hat{y} = 0.99$ â†’ "niskie zaskoczenie" = maÅ‚a kara
   - Model przewiduje $\hat{y} = 0.01$ â†’ "MEGA zaskoczenie!" = duÅ¼a kara
   
2. "Zaskoczenie" mierzymy jako $-\log(\hat{y})$
   - $-\log(0.99) = 0.01$ (maÅ‚a kara)
   - $-\log(0.01) = 4.6$ (duÅ¼a kara)

**PeÅ‚ny wzÃ³r dla obu przypadkÃ³w:**

Chcemy jednej formuÅ‚y, ktÃ³ra dziaÅ‚a zarÃ³wno dla $y=0$ jak i $y=1$:

- Gdy $y=1$: kara = $-\log(\hat{y})$
- Gdy $y=0$: kara = $-\log(1-\hat{y})$

Sprytna sztuczka matematyczna - poÅ‚Ä…czmy to:
$$L(y, \hat{y}) = -[y \cdot \log(\hat{y}) + (1-y) \cdot \log(1-\hat{y})]$$

**Dlaczego to dziaÅ‚a?**
- Gdy $y=1$: $(1-y)=0$, wiÄ™c drugi czÅ‚on znika â†’ zostaje $-\log(\hat{y})$ âœ“
- Gdy $y=0$: $y=0$, wiÄ™c pierwszy czÅ‚on znika â†’ zostaje $-\log(1-\hat{y})$ âœ“

**Dlaczego akurat logarytm?**
- Ma odpowiednie wÅ‚aÅ›ciwoÅ›ci matematyczne (rÃ³Å¼niczkowalny, wypukÅ‚y)
- Mocno karze pewne bÅ‚Ä™dy (gdy model jest bardzo pewny, a siÄ™ myli)
- Wynika z maksymalizacji prawdopodobieÅ„stwa (Maximum Likelihood Estimation)

#### ğŸ”¬ ZwiÄ…zek z prawdopodobieÅ„stwem (dla ciekawskich)

Model daje prawdopodobieÅ„stwo: $P(y=1|x) = \hat{y}$

Dla pojedynczej prÃ³bki prawdopodobieÅ„stwo poprawnej predykcji to:
$$P(y|x) = \hat{y}^y \cdot (1-\hat{y})^{1-y}$$

(Gdy y=1, zostaje $\hat{y}$; gdy y=0, zostaje $1-\hat{y}$)

Chcemy **maksymalizowaÄ‡** to prawdopodobieÅ„stwo. Bierzemy logarytm (Å‚atwiej matematycznie):
$$\log P(y|x) = y \log(\hat{y}) + (1-y) \log(1-\hat{y})$$

**Maksymalizacja** = **minimalizacja z minusem**, stÄ…d:
$$L = -[y \log(\hat{y}) + (1-y) \log(1-\hat{y})]$$

To wÅ‚aÅ›nie Binary Cross-Entropy!

### 5. Spadek gradientu - jak model siÄ™ uczy?

**Wielka idea:** WyobraÅº sobie, Å¼e stoisz w gÃ³rach w gÄ™stej mgle i chcesz zejÅ›Ä‡ do doliny (tam gdzie funkcja kosztu jest najmniejsza). Nie widzisz, gdzie jest dolina, ale czujesz nachylenie terenu pod stopami. **Idziesz w dÃ³Å‚!**

To wÅ‚aÅ›nie robi gradient descent:
1. Sprawdza "nachylenie" funkcji kosztu (gradient)
2. Robi krok w kierunku, gdzie koszt maleje
3. Powtarza, aÅ¼ znajdzie minimum (lub siÄ™ zmÄ™czy - max. liczba krokÃ³w)

**Algorytm krok po kroku:**

```
1. START: Ustaw losowe wagi w i bias b
2. PÄ˜TLA (dla kaÅ¼dej epoki):
   a) Policz predykcje dla wszystkich danych
   b) Policz funkcjÄ™ kosztu (jak bardzo siÄ™ mylisz)
   c) Policz gradient (w ktÃ³rÄ… stronÄ™ iÅ›Ä‡, Å¼eby poprawiÄ‡ wynik)
   d) AKTUALIZUJ parametry:
      w = w - Î± Ã— gradient_w
      b = b - Î± Ã— gradient_b
3. STOP: gdy koszt przestanie maleÄ‡ lub osiÄ…gniesz max. epok
```

**Co to jest Î± (learning rate)?**
To "dÅ‚ugoÅ›Ä‡ kroku". 
- Za duÅ¼e Î± â†’ przeskakujesz minimum, model skacze jak szalony ğŸ¤ª
- Za maÅ‚e Î± â†’ uczysz siÄ™ meeeeeedlennie, ale bezpiecznie ğŸŒ
- W samÄ… porÄ™ Î± â†’ szybko i dokÅ‚adnie dochodzisz do celu! ğŸ¯

### 6. Gradienty - matematyka, ktÃ³ra napÄ™dza uczenie

Gradient to po prostu "nachylenie" funkcji - mÃ³wi, jak szybko roÅ›nie funkcja w danym kierunku.

**Dla wag:**
$$\frac{\partial J}{\partial w} = \frac{1}{m} X^T (\hat{y} - y)$$

**Po ludzku:** 
- $(\hat{y} - y)$ to "bÅ‚Ä…d" - jak bardzo siÄ™ pomyliÅ‚eÅ›
- $X^T$ to cechy twojego pacjenta
- MnoÅ¼ysz je razem i dostajesz: "w ktÃ³rÄ… stronÄ™ i jak mocno zmieniÄ‡ wagi"

**Dla biasu:**
$$\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})$$

**Po ludzku:** To po prostu Å›redni bÅ‚Ä…d ze wszystkich prÃ³bek.

**Magia:** ZauwaÅ¼, Å¼e gradient zaleÅ¼y od bÅ‚Ä™du $(\hat{y} - y)$. Im wiÄ™kszy bÅ‚Ä…d, tym wiÄ™ksza korekta!

#### ğŸ”¬ SkÄ…d siÄ™ biorÄ… te wzory na gradienty?

**Zacznijmy od funkcji kosztu:**
$$J(w, b) = \frac{1}{m} \sum_{i=1}^{m} L(y^{(i)}, \hat{y}^{(i)})$$

gdzie: $\hat{y} = \sigma(w^T x + b)$

**Gradient wzglÄ™dem WAG (âˆ‚J/âˆ‚w):**

UÅ¼yjmy **reguÅ‚y Å‚aÅ„cuchowej**:
$$\frac{\partial J}{\partial w} = \frac{\partial J}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z} \cdot \frac{\partial z}{\partial w}$$

gdzie $z = w^T x + b$

**Krok 1:** Pochodna BCE wzglÄ™dem $\hat{y}$:
$$\frac{\partial L}{\partial \hat{y}} = -\frac{y}{\hat{y}} + \frac{1-y}{1-\hat{y}}$$

Upraszczamy do wspÃ³lnego mianownika:
$$\frac{\partial L}{\partial \hat{y}} = \frac{-y(1-\hat{y}) + (1-y)\hat{y}}{\hat{y}(1-\hat{y})} = \frac{\hat{y} - y}{\hat{y}(1-\hat{y})}$$

**Krok 2:** Pochodna sigmoid wzglÄ™dem $z$:
$$\frac{\partial \hat{y}}{\partial z} = \sigma'(z) = \sigma(z)(1-\sigma(z)) = \hat{y}(1-\hat{y})$$

**Krok 3:** Pochodna $z$ wzglÄ™dem $w$:
$$\frac{\partial z}{\partial w} = \frac{\partial (w^T x + b)}{\partial w} = x$$

**ÅÄ…czymy wszystko:**
$$\frac{\partial L}{\partial w} = \frac{\hat{y} - y}{\hat{y}(1-\hat{y})} \cdot \hat{y}(1-\hat{y}) \cdot x = (\hat{y} - y) \cdot x$$

**MAGIA:** Åšrodkowe czÅ‚ony siÄ™ skracajÄ…! $\hat{y}(1-\hat{y})$ w liczniku i mianowniku znika!

**Dla wszystkich prÃ³bek (macierzowo):**
$$\frac{\partial J}{\partial w} = \frac{1}{m} X^T (\hat{y} - y)$$

**Gradient wzglÄ™dem BIASU (âˆ‚J/âˆ‚b):**

Analogicznie:
$$\frac{\partial z}{\partial b} = 1$$

WiÄ™c:
$$\frac{\partial L}{\partial b} = (\hat{y} - y)$$

Dla wszystkich prÃ³bek:
$$\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})$$

**Elegancja tego wyniku:** Gradient to po prostu bÅ‚Ä…d razy wejÅ›cie! Sigmoid + BCE dajÄ… super proste gradienty.

### 7. Normalizacja - dlaczego to jest MEGA waÅ¼ne?

**Problem:** Masz cechy o rÃ³Å¼nych skalach:
- Wiek: 20-80 (zakres ~60)
- Cholesterol: 150-300 (zakres ~150)
- JakiÅ› wskaÅºnik: 0.001-0.01 (zakres ~0.009)

Model bÄ™dzie miaÅ‚ trudnoÅ›ci! Wagi dla cholesterolu bÄ™dÄ… mikroskopijne, a dla wskaÅºnika ogromne. Uczenie bÄ™dzie wolne i niestabilne.

**RozwiÄ…zanie: Standaryzacja**
$$x_{znormalizowane} = \frac{x - \text{Å›rednia}}{\text{odchylenie standardowe}}$$

**Co to robi?**
PrzeksztaÅ‚ca kaÅ¼dÄ… cechÄ™ tak, Å¼e:
- Ma Å›redniÄ… = 0 (jest "wycentrowana")
- Ma odchylenie standardowe = 1 (ma "standardowy zakres")

Teraz wszystkie cechy sÄ… porÃ³wnywalne!

**âš ï¸ SUPER WAÅ»NE:**
1. Oblicz Å›redniÄ… i odchylenie **TYLKO** na danych treningowych
2. UÅ¼yj TYCH SAMYCH wartoÅ›ci do normalizacji danych testowych
3. Dlaczego? Bo w Å¼yciu codziennym model bÄ™dzie widziaÅ‚ nowe dane - nie moÅ¼e "podglÄ…daÄ‡" ich statystyk z gÃ³ry!


### 8. Metryki - jak oceniÄ‡ jakoÅ›Ä‡ modelu?

Po wytrenowaniu modelu trzeba go sprawdziÄ‡. "Accuracy to za maÅ‚o!" - usÅ‚yszysz czÄ™sto. Dlaczego?

**PrzykÅ‚ad problemu:**
Masz 100 pacjentÃ³w, 95 zdrowych, 5 chorych. Model-idiota mÃ³wi zawsze "zdrowy".
- Accuracy = 95%! Wow! ğŸ‰
- Ale... nie zÅ‚apaÅ‚ ANI JEDNEGO chorego! ğŸ˜±

Dlatego patrzymy na wiÄ™cej metryk:

#### Accuracy (dokÅ‚adnoÅ›Ä‡)
$$\text{Accuracy} = \frac{\text{ile dobrze przewidziaÅ‚}}{\text{ile byÅ‚o wszystkich}}$$

**Intuicja:** OgÃ³lna "celnoÅ›Ä‡" modelu. Dobra jako pierwszy sprawdzian, ale nie mÃ³w caÅ‚ej prawdy.

#### Precision (precyzja)
$$\text{Precision} = \frac{TP}{TP + FP}$$

**Po ludzku:** "SpoÅ›rÃ³d tych, ktÃ³rych model oznaczyÅ‚ jako CHORYCH, ilu rzeczywiÅ›cie jest chorych?"

- Wysoka precyzja = jak model mÃ³wi "chory", to raczej ma racjÄ™
- Niska precyzja = duÅ¼o faÅ‚szywych alarmÃ³w

**Kiedy waÅ¼na?** Gdy faÅ‚szywy alarm jest kosztowny (np. niepotrzebna operacja).

#### Recall / CzuÅ‚oÅ›Ä‡
$$\text{Recall} = \frac{TP}{TP + FN}$$

**Po ludzku:** "SpoÅ›rÃ³d wszystkich RZECZYWIÅšCIE CHORYCH, ilu model zÅ‚apaÅ‚?"

- Wysoki recall = model Å‚apie wiÄ™kszoÅ›Ä‡ chorych
- Niski recall = model przegapia chorych ludzi

**Kiedy waÅ¼na?** Gdy przeoczenie chorego jest niebezpieczne (np. wykrywanie raka).

#### F1-score (kompromis)
$$\text{F1} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$$

**Po ludzku:** Åšrednia harmoniczna precision i recall. ÅÄ…czy obie metryki w jednÄ… liczbÄ™.

Wysoki F1 = model jest jednoczeÅ›nie dokÅ‚adny I Å‚apie wiÄ™kszoÅ›Ä‡ przypadkÃ³w. To dobra rÃ³wnowaga!

**Przypomnienie:**
- TP (True Positive) = poprawnie przewidziany chory
- FP (False Positive) = pomyÅ‚ka - zdrowy oznaczony jako chory
- FN (False Negative) = pomyÅ‚ka - chory oznaczony jako zdrowy
- TN (True Negative) = poprawnie przewidziany zdrowy

### 9. PrÃ³g decyzyjny - kiedy mÃ³wiÄ‡ TAK?

Model daje ci prawdopodobieÅ„stwo, np. 0.73. Ale potrzebujesz decyzji: chory czy zdrowy?

**Standardowo:** PrÃ³g = 0.5
- p â‰¥ 0.5 â†’ CHORY (klasa 1)
- p < 0.5 â†’ ZDROWY (klasa 0)

**Ale moÅ¼esz to zmieniaÄ‡!**
- PrÃ³g = 0.3 â†’ Model chÄ™tniej mÃ³wi "chory" (wiÄ™cej faÅ‚szywych alarmÃ³w, ale mniej przeoczonych chorych)
- PrÃ³g = 0.7 â†’ Model ostroÅ¼niejszy (mniej faÅ‚szywych alarmÃ³w, ale wiÄ™cej przeoczonych chorych)

**Jak wybraÄ‡ prÃ³g?** ZaleÅ¼y od problemu:
- Wykrywanie raka? Lepiej faÅ‚szywy alarm niÅ¼ przeoczona choroba â†’ niÅ¼szy prÃ³g
- Spam w mailu? Lepiej czasem przepuÅ›ciÄ‡ spam niÅ¼ usunÄ…Ä‡ waÅ¼ny mail â†’ wyÅ¼szy prÃ³g

---

## Laboratorium 3 - Wielowarstwowa SieÄ‡ Neuronowa

### 1. Czym rÃ³Å¼ni siÄ™ sieÄ‡ wielowarstwowa od regresji logistycznej?

**Regresja logistyczna (Lab 2):** Prosty "neuron"
```
wejÅ›cie â†’ [wagi i bias] â†’ sigmoid â†’ wyjÅ›cie
```
**Umie:** Tylko proste, liniowe decyzje (np. prosta linia dzielÄ…ca zdrowych od chorych)

**SieÄ‡ wielowarstwowa (MLP - Lab 3):** Stos neuronÃ³w uÅ‚oÅ¼onych w warstwy!
```
wejÅ›cie â†’ [warstwa 1] â†’ [warstwa 2] â†’ [warstwa 3] â†’ wyjÅ›cie
```
**Umie:** ZÅ‚oÅ¼one, nieliniowe wzorce (moÅ¼e rysowaÄ‡ zakrzywione granice, wykrywaÄ‡ skomplikowane zaleÅ¼noÅ›ci)

**Analogia:** 
- Regresja logistyczna = kalkulator
- MLP = komputer

### 2. Anatomia sieci wielowarstwowej

**Warstwy:**
1. **Warstwa wejÅ›ciowa** - po prostu twoje dane (np. wiek, ciÅ›nienie, cholesterol)
2. **Warstwy ukryte** - tu dzieje siÄ™ magia! To "myÅ›lÄ…ce" warstwy, ktÃ³re wykrywajÄ… wzorce
3. **Warstwa wyjÅ›ciowa** - finalna predykcja

**Co dzieje siÄ™ w kaÅ¼dej warstwie?**
```
[poprzednia warstwa] 
    â†“
1. MnoÅ¼enie przez wagi + dodanie biasu
    â†“
2. Funkcja aktywacji (nieliniowoÅ›Ä‡!)
    â†“
[aktualna warstwa]
```

**Dlaczego wiele warstw = wiÄ™ksza moc?**
- Pierwsza warstwa ukryta: wykrywa proste wzorce ("tutaj jest krawÄ™dÅº", "tu coÅ› siÄ™ zmienia")
- Druga warstwa: Å‚Ä…czy proste wzorce w bardziej zÅ‚oÅ¼one ("to wyglÄ…da jak nos", "to przypomina krzywÄ…")
- Trzecia warstwa: jeszcze bardziej abstrakcyjne koncepty
- WyjÅ›cie: finalna decyzja oparta na wszystkich wykrytych wzorcach

### 3. Matematyka pojedynczej warstwy (Å‚atwa!)

**Krok 1: Transformacja liniowa**
$$z = xW + b$$

- $x$ - to co wchodzi (np. [wiek, ciÅ›nienie, cholesterol])
- $W$ - wagi (macierz "poÅ‚Ä…czeÅ„" miÄ™dzy wejÅ›ciami a neuronami)
- $b$ - biasy (osobne "przesuniÄ™cie" dla kaÅ¼dego neuronu)
- $z$ - wynik przed aktywacjÄ…

**Krok 2: Funkcja aktywacji**
$$a = f(z)$$

Ta funkcja wprowadza "nieliniowoÅ›Ä‡" - bez niej caÅ‚a sieÄ‡ zachowywaÅ‚aby siÄ™ jak jedna duÅ¼a regresja logistyczna!

### 4. Funkcje aktywacji - kluczowe skÅ‚adniki

#### 4.1 Sigmoid (znasz juÅ¼ z Lab 2!)
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

**Kiedy uÅ¼ywaÄ‡?** 
- Warstwa wyjÅ›ciowa w klasyfikacji binarnej (daje prawdopodobieÅ„stwo 0-1)

**Problem w warstwach ukrytych:**
Dla bardzo duÅ¼ych lub maÅ‚ych z, pochodna jest bliska 0. Gradient "zanika" - sieÄ‡ uczy siÄ™ mega wolno lub wcale!

#### 4.2 ReLU (Rectified Linear Unit) - gwiazda gÅ‚Ä™bokich sieci!
$$\text{ReLU}(z) = \max(0, z) = \begin{cases} z & \text{jeÅ›li } z > 0 \\ 0 & \text{jeÅ›li } z \leq 0 \end{cases}$$

**Intuicja:** "Przepuszczasz tylko wartoÅ›ci dodatnie, resztÄ™ zeruj"

**Pochodna:**
$$\text{ReLU}'(z) = \begin{cases} 1 & \text{jeÅ›li } z > 0 \\ 0 & \text{w przeciwnym razie} \end{cases}$$

#### ğŸ”¬ SkÄ…d siÄ™ bierze wzÃ³r na pochodnÄ… ReLU?

ReLU to najprostrza funkcja w historii gÅ‚Ä™bokich sieci!

$$\text{ReLU}(z) = \max(0, z) = \begin{cases} z & \text{jeÅ›li } z > 0 \\ 0 & \text{jeÅ›li } z \leq 0 \end{cases}$$

**Pochodna to po prostu nachylenie:**

**Dla z > 0:** Funkcja to po prostu $f(z) = z$, wiÄ™c $f'(z) = 1$
**Dla z < 0:** Funkcja to staÅ‚a $f(z) = 0$, wiÄ™c $f'(z) = 0$
**Dla z = 0:** Technicznie pochodna nie istnieje (zaÅ‚amanie), ale w praktyce przyjmujemy 0 lub 1 (zazwyczaj 0)

$$\boxed{\text{ReLU}'(z) = \begin{cases} 1 & \text{jeÅ›li } z > 0 \\ 0 & \text{w przeciwnym razie} \end{cases}}$$

**Dlaczego to jest genialne?**
- Obliczeniowo darmowe - tylko porÃ³wnanie z zerem!
- Gradient = 1 dla aktywnych neuronÃ³w (nie zanika!)
- Gradient = 0 dla nieaktywnych (neuron "wyÅ‚Ä…czony")

To jest najprostsza moÅ¼liwa nieliniowoÅ›Ä‡, ktÃ³ra dziaÅ‚a!

**Dlaczego jest super?**
âœ… Prosta jak budowa cepa
âœ… Szybka do obliczenia
âœ… Nie ma problemu zanikajÄ…cego gradientu (dla z > 0)
âœ… Empirycznie dziaÅ‚a Å›wietnie w gÅ‚Ä™bokich sieciach

**Jeden problem: "Dying ReLU"**
JeÅ›li neuron wpadnie w rejon z < 0 i tam zostanie, jego pochodna = 0. "Umiera" - przestaje siÄ™ uczyÄ‡.

**RozwiÄ…zanie:** Leaky ReLU, ale to juÅ¼ dla zaawansowanych ğŸ˜

**Kiedy uÅ¼ywaÄ‡?**
- Warstwy ukryte - praktycznie zawsze!
- Chyba Å¼e masz dobry powÃ³d, Å¼eby uÅ¼yÄ‡ czegoÅ› innego


### 5. Forward Propagation - "przepychanie" danych przez sieÄ‡

**To jest prosta czÄ™Å›Ä‡!** Bierzesz dane i przepuszczasz je przez kolejne warstwy. KaÅ¼da warstwa robi swoje: mnoÅ¼y, dodaje, aktywuje.

**PrzykÅ‚ad z 3 warstwami:**

```
Dane wejÅ›ciowe: [wiek, ciÅ›nienie, cholesterol, ...]
    â†“
WARSTWA 1 (ukryta, 32 neurony, ReLU):
    zÂ¹ = xÂ·WÂ¹ + bÂ¹
    aÂ¹ = ReLU(zÂ¹)
    â†“
WARSTWA 2 (ukryta, 16 neuronÃ³w, ReLU):
    zÂ² = aÂ¹Â·WÂ² + bÂ²
    aÂ² = ReLU(zÂ²)
    â†“
WARSTWA 3 (wyjÅ›cie, 1 neuron, Sigmoid):
    zÂ³ = aÂ²Â·WÂ³ + bÂ³
    Å· = Sigmoid(zÂ³)
    â†“
Wynik: 0.73 (73% prawdopodobieÅ„stwa choroby)
```

**Klucz:** WyjÅ›cie z jednej warstwy (a) staje siÄ™ wejÅ›ciem do nastÄ™pnej!

**W kodzie:**
```python
def forward(X):
    out = X
    for warstwa in warstwy:
        out = warstwa.forward(out)
    return out  # finalna predykcja
```

### 6. Backpropagation - MAGIA uczenia sieci!

**To jest trudniejsza czÄ™Å›Ä‡, ale najwaÅ¼niejsza!**

OK, mamy sieÄ‡. Robi predykcje. Ale **jak jÄ… nauczyÄ‡?** Musimy zaktualizowaÄ‡ wagi we WSZYSTKICH warstwach. Problem: jak gradient z wyjÅ›cia dotrzeÄ‡ do pierwszych warstw?

**OdpowiedÅº: Backpropagation** = propagacja wstecz = puszczanie gradientu od koÅ„ca do poczÄ…tku sieci.

#### Intuicja: gra w "gÅ‚uchy telefon" z gradientami

WyobraÅº sobie:
1. Na koÅ„cu (wyjÅ›cie) obliczamy: "o ile siÄ™ pomyliÅ‚em?"
2. Pytamy ostatniÄ… warstwÄ™: "o ile TY powinna zmieniÄ‡ swoje wagi?"
3. Ta warstwa mÃ³wi poprzedniej: "hej, twoja wina byÅ‚a taka-a-taka"
4. I tak dalej, aÅ¼ dotrzemy do poczÄ…tku

KaÅ¼da warstwa:
- Dostaje "winÄ™" z kolejnej warstwy (gradient)
- Oblicza, jak zmieniÄ‡ swoje wagi
- Przekazuje czÄ™Å›Ä‡ "winy" do warstwy przed sobÄ…

#### Matematyka (uproszczona)

**Dla warstwy $l$:**

**Krok 1:** Masz gradient wzglÄ™dem aktywacji: $\frac{\partial L}{\partial a^{[l]}}$ 
(to jest "wina" przekazana przez nastÄ™pnÄ… warstwÄ™)

**Krok 2:** Oblicz gradient wzglÄ™dem z (przed aktywacjÄ…):
$$\frac{\partial L}{\partial z^{[l]}} = \frac{\partial L}{\partial a^{[l]}} \odot f'(z^{[l]})$$

$\odot$ = mnoÅ¼enie element po elemencie (kaÅ¼dy z kaÅ¼dym z osobna)

**Dlaczego pochodna aktywacji?** Bo to "bramka" - kontroluje, jak mocno sygnaÅ‚ przeszedÅ‚. JeÅ›li $f'(z)$ jest maÅ‚a, gradient sÅ‚abnie (problem zanikajÄ…cego gradientu!).

**Krok 3:** Oblicz, jak zmieniÄ‡ WAGI tej warstwy:
$$\frac{\partial L}{\partial W^{[l]}} = \frac{1}{m} (a^{[l-1]})^T \cdot \frac{\partial L}{\partial z^{[l]}}$$

**Intuicja:** "KtÃ³re wagi byÅ‚y najbardziej odpowiedzialne za bÅ‚Ä…d?"
- To zaleÅ¼y od aktywacji wejÅ›ciowej ($a^{[l-1]}$) i bÅ‚Ä™du ($\frac{\partial L}{\partial z^{[l]}}$)
- DuÅ¼e wejÅ›cie Ã— duÅ¼y bÅ‚Ä…d = ta waga potrzebuje duÅ¼ej korekty!

**Krok 4:** Oblicz gradient dla biasÃ³w:
$$\frac{\partial L}{\partial b^{[l]}} = \frac{1}{m} \sum \frac{\partial L}{\partial z^{[l]}}$$

Po prostu Å›redni bÅ‚Ä…d z wszystkich prÃ³bek.

**Krok 5:** PrzekaÅ¼ "winÄ™" do poprzedniej warstwy:
$$\frac{\partial L}{\partial a^{[l-1]}} = \frac{\partial L}{\partial z^{[l]}} \cdot (W^{[l]})^T$$

To bÄ™dzie gradient dla warstwy $l-1$. I tak w kÃ³Å‚ko, aÅ¼ dojdziesz do poczÄ…tku!

#### ğŸ”¬ SkÄ…d siÄ™ biorÄ… wzory w backpropagation?

To wyglÄ…da na czarnÄ… magiÄ™, ale to tylko **reguÅ‚a Å‚aÅ„cuchowa** zastosowana wielokrotnie!

**Przypomnijmy reguÅ‚Ä™ Å‚aÅ„cuchowÄ…:**
JeÅ›li $y = f(g(x))$, to: $\frac{dy}{dx} = \frac{dy}{dg} \cdot \frac{dg}{dx}$

**W naszej sieci:**
```
x â†’ [Warstwa 1] â†’ aÂ¹ â†’ [Warstwa 2] â†’ aÂ² â†’ ... â†’ Å· â†’ [Loss] â†’ L
```

Chcemy: $\frac{\partial L}{\partial W^{[l]}}$ (jak zmiana wag wpÅ‚ywa na loss)

**Wyprowadzenie dla warstwy $l$:**

Oznaczenia:
- $z^{[l]} = a^{[l-1]} W^{[l]} + b^{[l]}$ (przed aktywacjÄ…)
- $a^{[l]} = f(z^{[l]})$ (po aktywacji)

**1) Gradient wzglÄ™dem z (pre-activation):**

Wiemy, Å¼e aktywacja dziaÅ‚a na kaÅ¼dy element z osobna:
$$a^{[l]}_i = f(z^{[l]}_i)$$

ReguÅ‚a Å‚aÅ„cuchowa:
$$\frac{\partial L}{\partial z^{[l]}} = \frac{\partial L}{\partial a^{[l]}} \cdot \frac{\partial a^{[l]}}{\partial z^{[l]}}$$

To $\frac{\partial a^{[l]}}{\partial z^{[l]}}$ to po prostu $f'(z^{[l]})$ - pochodna funkcji aktywacji!

$$\boxed{\frac{\partial L}{\partial z^{[l]}} = \frac{\partial L}{\partial a^{[l]}} \odot f'(z^{[l]})}$$

($\odot$ = mnoÅ¼enie element-wise, bo kaÅ¼dy element z ma swojÄ… pochodnÄ…)

**2) Gradient wzglÄ™dem WAG:**

$$z^{[l]} = a^{[l-1]} W^{[l]} + b^{[l]}$$

To mnoÅ¼enie macierzy! Dla pojedynczego elementu:
$$z^{[l]}_{ij} = \sum_k a^{[l-1]}_{ik} W^{[l]}_{kj}$$

Pochodna wzglÄ™dem wagi $W_{kj}$:
$$\frac{\partial z^{[l]}_{ij}}{\partial W^{[l]}_{kj}} = a^{[l-1]}_{ik}$$

W notacji macierzowej (uÅ¼ywajÄ…c wÅ‚aÅ›ciwoÅ›ci iloczynu macierzy):
$$\boxed{\frac{\partial L}{\partial W^{[l]}} = \frac{1}{m} (a^{[l-1]})^T \frac{\partial L}{\partial z^{[l]}}}$$

**Intuicja:** 
- $(a^{[l-1]})^T$ - co weszÅ‚o do warstwy
- $\frac{\partial L}{\partial z^{[l]}}$ - jak bardzo siÄ™ pomyliÅ‚a
- MnoÅ¼enie: ktÃ³re poÅ‚Ä…czenia (wagi) byÅ‚y odpowiedzialne za bÅ‚Ä…d

**3) Gradient wzglÄ™dem BIASU:**

$$\frac{\partial z^{[l]}}{\partial b^{[l]}} = 1$$

Bo bias dodajemy bezpoÅ›rednio. WiÄ™c:
$$\boxed{\frac{\partial L}{\partial b^{[l]}} = \frac{1}{m} \sum \frac{\partial L}{\partial z^{[l]}}}$$

Po prostu sumujemy gradienty po wszystkich przykÅ‚adach (bo ten sam bias jest uÅ¼ywany wszÄ™dzie).

**4) Gradient przekazywany wstecz:**

Potrzebujemy $\frac{\partial L}{\partial a^{[l-1]}}$ Å¼eby mÃ³c policzyÄ‡ gradienty dla poprzedniej warstwy.

Z rÃ³wnania: $z^{[l]} = a^{[l-1]} W^{[l]} + b^{[l]}$

ReguÅ‚a Å‚aÅ„cuchowa:
$$\frac{\partial L}{\partial a^{[l-1]}} = \frac{\partial L}{\partial z^{[l]}} \cdot \frac{\partial z^{[l]}}{\partial a^{[l-1]}}$$

Pochodna wzglÄ™dem $a^{[l-1]}$:
$$\frac{\partial z^{[l]}}{\partial a^{[l-1]}} = W^{[l]}$$

W notacji macierzowej:
$$\boxed{\frac{\partial L}{\partial a^{[l-1]}} = \frac{\partial L}{\partial z^{[l]}} (W^{[l]})^T}$$

**Intuicja:** "Wina" rozprzestrzenia siÄ™ wstecz przez te same poÅ‚Ä…czenia (wagi), ktÃ³rymi szedÅ‚ sygnaÅ‚ do przodu!

#### ğŸ”¬ Dlaczego dla Sigmoid + BCE wychodzi tak Å‚adnie?

**Twierdzenie:** Dla ostatniej warstwy z sigmoid i BCE:
$$\frac{\partial L}{\partial z^{[last]}} = \hat{y} - y$$

**DowÃ³d:**

Funkcja kosztu: $L = -[y \log(\hat{y}) + (1-y) \log(1-\hat{y})]$

gdzie $\hat{y} = \sigma(z)$

**Krok 1:** Pochodna L wzglÄ™dem $\hat{y}$:
$$\frac{\partial L}{\partial \hat{y}} = -\frac{y}{\hat{y}} + \frac{1-y}{1-\hat{y}} = \frac{\hat{y} - y}{\hat{y}(1-\hat{y})}$$

**Krok 2:** Pochodna sigmoid:
$$\frac{\partial \hat{y}}{\partial z} = \sigma(z)(1-\sigma(z)) = \hat{y}(1-\hat{y})$$

**Krok 3:** ÅÄ…czymy (reguÅ‚a Å‚aÅ„cuchowa):
$$\frac{\partial L}{\partial z} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z} = \frac{\hat{y} - y}{\hat{y}(1-\hat{y})} \cdot \hat{y}(1-\hat{y})$$

**MAGIA:** $\hat{y}(1-\hat{y})$ siÄ™ skraca!

$$\boxed{\frac{\partial L}{\partial z} = \hat{y} - y}$$

**To nie przypadek!** Sigmoid i BCE zostaÅ‚y "stworzone dla siebie". Kombinacja daje najprostszy moÅ¼liwy gradient.

To samo dzieje siÄ™ dla:
- Softmax + Categorical Cross-Entropy (dla wielu klas)
- MSE + Identity (dla regresji)

#### Specjalny przypadek: ostatnia warstwa z BCE i Sigmoid

Normalna jest tam straszna matematyka z reguÅ‚y Å‚aÅ„cuchowej. ALE! Jest piÄ™kne uproszczenie:

$$\frac{\partial L}{\partial z^{[last]}} = \hat{y} - y$$

**TAK! Po prostu bÅ‚Ä…d predykcji!** 

To wynika z magicznych wÅ‚aÅ›ciwoÅ›ci sigmoid + BCE. Pochodne siÄ™ upraszczajÄ… i zostaje czysta rÃ³Å¼nica.

### 7. Aktualizacja parametrÃ³w

Gdy masz juÅ¼ wszystkie gradienty (z backprop), aktualizujesz wagi:

$$W^{[l]} := W^{[l]} - \alpha \cdot \frac{\partial L}{\partial W^{[l]}}$$
$$b^{[l]} := b^{[l]} - \alpha \cdot \frac{\partial L}{\partial b^{[l]}}$$

Znak minus, bo idziesz PRZECIWNIE do gradientu (w dÃ³Å‚, nie w gÃ³rÄ™!)

### 8. PeÅ‚ny cykl treningu - wszystko razem

```
SETUP: Zainicjuj losowe wagi dla wszystkich warstw

PÄ˜TLA TRENINGOWA (epochs):
    
    1. FORWARD PASS:
       - PrzepuÅ›Ä‡ dane przez sieÄ‡ (warstwa po warstwie)
       - Zapisz wszystkie poÅ›rednie wartoÅ›ci (z, a)
       - Oblicz predykcjÄ™ Å·
       - Oblicz funkcjÄ™ kosztu L(y, Å·)
    
    2. BACKWARD PASS:
       - Zacznij od koÅ„ca: policz gradient wyjÅ›cia
       - IdÅº wstecz przez warstwy:
         * Policz gradienty dla wag i biasÃ³w
         * PrzekaÅ¼ gradient do poprzedniej warstwy
    
    3. UPDATE:
       - Zaktualizuj wszystkie wagi: W -= Î± Ã— gradient
       - Zaktualizuj wszystkie biasy: b -= Î± Ã— gradient
    
    4. SPRAWDÅ¹:
       - Czy koszt maleje? âœ“ Dobrze!
       - Czy przestaÅ‚ maleÄ‡? â†’ STOP, juÅ¼ nauczony

KONIEC: Masz wytrenowanÄ… sieÄ‡!
```

### 9. Inicjalizacja wag - zacznij dobrze!

**Nie moÅ¼esz zainicjowaÄ‡ wszystkiego zerami!** Bo wtedy wszystkie neurony w warstwie bÄ™dÄ… robiÄ‡ to samo i uczÄ… siÄ™ jednakowo. Symetria = marnowanie neuronÃ³w!

**Losowe maÅ‚e wartoÅ›ci:**
- NajproÅ›ciej: `W = np.random.randn(n_in, n_out) * 0.01`

**He initialization (dla ReLU):**
$$W \sim \mathcal{N}(0, \sqrt{\frac{2}{n_{in}}})$$

Bierze pod uwagÄ™ liczbÄ™ wejÅ›Ä‡ - im wiÄ™cej, tym mniejsze wagi (Å¼eby nie wybuchnÄ™Å‚y).

**Xavier/Glorot (dla Sigmoid/Tanh):**
$$W \sim \mathcal{N}(0, \sqrt{\frac{1}{n_{in}}})$$

**Biasy:**
Zazwyczaj zera sÄ… OK. Czasem maÅ‚e wartoÅ›ci dodatnie dla ReLU.

**Dlaczego to waÅ¼ne?**
- Za duÅ¼e wagi â†’ aktywacje eksplodujÄ…, gradient eksploduje, uczenie siÄ™ wypalam
- Za maÅ‚e wagi â†’ aktywacje umierajÄ…, gradient zanika, sieÄ‡ nie uczy siÄ™ nic
- W sam raz â†’ uczenie przebiega gÅ‚adko ğŸ¯

#### ğŸ”¬ SkÄ…d siÄ™ biorÄ… te konkretne wartoÅ›ci inicjalizacji?

**Problem do rozwiÄ…zania:** Chcemy, Å¼eby wariancja aktywacji byÅ‚a podobna w kaÅ¼dej warstwie. Bez tego:
- GÅ‚Ä™bokie warstwy mogÄ… mieÄ‡ gigantyczne wartoÅ›ci (exploding)
- Lub mikroskopijne wartoÅ›ci bliskie zeru (vanishing)

**He Initialization (dla ReLU):**

ZaÅ‚Ã³Å¼my, Å¼e:
- WejÅ›cie $x$ ma wariancjÄ™ $Var(x) = 1$ (po normalizacji)
- Mamy $n_{in}$ wejÅ›Ä‡ do neuronu
- Wagi: $w_i \sim \mathcal{N}(0, \sigma_w^2)$

WyjÅ›cie neuronu (przed aktywacjÄ…):
$$z = w_1 x_1 + w_2 x_2 + ... + w_{n_{in}} x_{n_{in}}$$

**Wariancja sumy niezaleÅ¼nych zmiennych losowych:**
$$Var(z) = Var(w_1 x_1) + Var(w_2 x_2) + ... + Var(w_{n_{in}} x_{n_{in}})$$

Dla kaÅ¼dego skÅ‚adnika (zakÅ‚adajÄ…c niezaleÅ¼noÅ›Ä‡):
$$Var(w_i x_i) = E[w_i]^2 Var(x_i) + E[x_i]^2 Var(w_i) + Var(w_i)Var(x_i)$$

Ale $E[w_i] = 0$ i $E[x_i] = 0$ (bo wycentrowane), wiÄ™c:
$$Var(w_i x_i) = Var(w_i) \cdot Var(x_i) = \sigma_w^2 \cdot 1 = \sigma_w^2$$

Zatem:
$$Var(z) = n_{in} \cdot \sigma_w^2$$

**Chcemy:** $Var(z) \approx 1$ (Å¼eby nie eksplodowaÅ‚o ani nie zanikÅ‚o)

WiÄ™c:
$$n_{in} \cdot \sigma_w^2 = 1$$
$$\sigma_w^2 = \frac{1}{n_{in}}$$
$$\sigma_w = \sqrt{\frac{1}{n_{in}}}$$

**Ale to dla liniowej aktywacji!** ReLU zeruje poÅ‚owÄ™ neuronÃ³w (z < 0), wiÄ™c:
- Efektywna liczba aktywnych wejÅ›Ä‡ to $\frac{n_{in}}{2}$
- Å»eby skompensowaÄ‡, mnoÅ¼ymy wariancjÄ™ przez 2

$$\boxed{\sigma_w = \sqrt{\frac{2}{n_{in}}}}$$

To jest **He initialization**!

**Xavier/Glorot Initialization (dla Sigmoid/Tanh):**

Dla sigmoid/tanh bierzemy pod uwagÄ™ zarÃ³wno forward jak i backward pass:
$$\sigma_w = \sqrt{\frac{1}{n_{in}}}$$

Lub uÅ›rednionÄ… wersjÄ™:
$$\sigma_w = \sqrt{\frac{2}{n_{in} + n_{out}}}$$

**W praktyce:** UÅ¼ywaj He dla ReLU, Xavier dla sigmoid/tanh!

### 10. Hiperparametry - pokrÄ™tÅ‚a do krÄ™cenia

**Learning rate (Î±):**
- Za maÅ‚y (np. 0.0001): Uczenie meeeedlennie, ale stabilnie
- Za duÅ¼y (np. 10): Chaos! SieÄ‡ nie moÅ¼e siÄ™ ustabilizowaÄ‡
- W sam raz (np. 0.01, 0.001): Szybko i sprawnie
- **Trick:** Zacznij od wiÄ™kszego, potem zmniejszaj (learning rate decay)

**Architektura (warstwy i neurony):**
- WiÄ™cej warstw = gÅ‚Ä™bsza sieÄ‡ = wiÄ™cej pojemnoÅ›ci = moÅ¼e uczyÄ‡ siÄ™ bardziej zÅ‚oÅ¼onych rzeczy
- ALE: Trudniejsza w trenowaniu, Å‚atwiej przeuczyÄ‡
- **ReguÅ‚a kciuka:** Zacznij od prostej (2-3 warstwy ukryte), zwiÄ™kszaj jeÅ›li potrzeba

**Liczba neuronÃ³w:**
- WiÄ™cej neuronÃ³w = wiÄ™ksza pojemnoÅ›Ä‡ warstwy
- ALE: WiÄ™cej obliczeÅ„, ryzyko przeuczenia
- **ReguÅ‚a kciuka:** Zaczynaj od coÅ› miÄ™dzy liczbÄ… cech wejÅ›ciowych a wyjÅ›ciowych

**Liczba epok:**
- Za maÅ‚o: Nie nauczy siÄ™ dobrze (underfitting)
- Za duÅ¼o: PrzepamiÄ™ta dane treningowe (overfitting)
- **ReguÅ‚a:** Trenuj, aÅ¼ loss przestanie maleÄ‡ na zbiorze walidacyjnym (early stopping)

**Funkcje aktywacji:**
- Warstwy ukryte: ReLU (lub jego warianty)
- WyjÅ›cie binarne: Sigmoid
- WyjÅ›cie multi-class: Softmax
- Regresja: Brak aktywacji (liniowe wyjÅ›cie)

### 11. Problem zanikajÄ…cego gradientu - dlaczego sigmoid w warstwach ukrytych to zÅ‚y pomysÅ‚

**Co siÄ™ dzieje?**

W backpropagation mnoÅ¼ysz gradienty przez pochodne funkcji aktywacji.

Dla sigmoid: $\sigma'(z) = \sigma(z)(1-\sigma(z))$
- Maksimum w z=0: $\sigma'(0) = 0.25$
- Dla duÅ¼ych |z|: $\sigma'(z) \approx 0$

**Problem:**
```
Warstwa 5: gradient Ã— 0.1
Warstwa 4: gradient Ã— 0.1 Ã— 0.2 = gradient Ã— 0.02
Warstwa 3: gradient Ã— 0.02 Ã— 0.15 = gradient Ã— 0.003
Warstwa 2: gradient Ã— 0.003 Ã— 0.1 = gradient Ã— 0.0003
Warstwa 1: gradient Ã— 0.0003 Ã— 0.2 = gradient Ã— 0.00006
```

Gradient ZANIKA! Pierwsze warstwy uczÄ… siÄ™ mega wolno lub wcale.

**RozwiÄ…zanie: ReLU!**
- Dla z > 0: pochodna = 1 (nie zmniejsza gradientu!)
- Gradient nie zanika tak Å‚atwo
- Dlatego ReLU = standard w gÅ‚Ä™bokich sieciach

#### ğŸ”¬ Matematyczne wyjaÅ›nienie zanikajÄ…cego gradientu

**Przypomnijmy backpropagation:**

Dla warstwy $l$:
$$\frac{\partial L}{\partial z^{[l]}} = \frac{\partial L}{\partial a^{[l]}} \odot f'(z^{[l]})$$

Propagacja do poprzedniej warstwy:
$$\frac{\partial L}{\partial a^{[l-1]}} = \frac{\partial L}{\partial z^{[l]}} (W^{[l]})^T$$

**W gÅ‚Ä™bokiej sieci (L warstw):**

Gradient dla pierwszej warstwy to iloczyn pochodnych ze WSZYSTKICH warstw:
$$\frac{\partial L}{\partial W^{[1]}} \propto f'^{[L]}(z^{[L]}) \cdot W^{[L]} \cdot f'^{[L-1]}(z^{[L-1]}) \cdot W^{[L-1]} \cdot ... \cdot f'^{[2]}(z^{[2]}) \cdot W^{[2]}$$

**Problem z sigmoid:**

Maksymalna wartoÅ›Ä‡ pochodnej sigmoid: $\sigma'(z) = \sigma(z)(1-\sigma(z)) \leq 0.25$

Dla 5 warstw:
$$\text{gradient} \propto 0.25 \times W \times 0.25 \times W \times 0.25 \times W \times ...$$

Nawet jeÅ›li wagi sÄ… OK (bliskie 1), mnoÅ¼ysz przez $0.25^5 = 0.00098$!

**Gradient maleje wykÅ‚adniczo z gÅ‚Ä™bokoÅ›ciÄ… sieci!**

**Dlaczego ReLU rozwiÄ…zuje problem:**

Dla $z > 0$: $\text{ReLU}'(z) = 1$

$$\text{gradient} \propto 1 \times W \times 1 \times W \times 1 \times W \times ...$$

Gradient NIE zanika automatycznie! (OczywiÅ›cie nadal moÅ¼e zanikaÄ‡ przez wagi, ale nie przez aktywacjÄ™)

**Dodatkowe sposoby walki:**
- Batch Normalization (normalizuje aktywacje miÄ™dzy warstwami)
- Residual connections / Skip connections (gradient moÅ¼e "ominÄ…Ä‡" warstwy)
- Gradient clipping (obcina za duÅ¼e gradienty)
- Lepsze inicjalizacje wag (He/Xavier)

### 12. PorÃ³wnanie: Regresja logistyczna vs MLP

| **Co?** | **Regresja logistyczna** | **MLP (sieÄ‡ wielowarstwowa)** |
|---------|-------------------------|-------------------------------|
| **Struktura** | Pojedynczy "neuron" | Wiele warstw neuronÃ³w |
| **ZÅ‚oÅ¼onoÅ›Ä‡** | Prosta, liniowa decyzja | MoÅ¼e uczyÄ‡ siÄ™ nieliniowych wzorcÃ³w |
| **Granica decyzyjna** | Prosta linia (lub pÅ‚aszczyzna) | Zakrzywione, zÅ‚oÅ¼one ksztaÅ‚ty |
| **Parametry** | Niewiele (n+1) | DuÅ¼o (zaleÅ¼y od architektury) |
| **SzybkoÅ›Ä‡ treningu** | Szybka | Wolniejsza |
| **Ryzyko overfittingu** | MaÅ‚e | WiÄ™ksze (potrzeba regularyzacji) |
| **Kiedy uÅ¼ywaÄ‡?** | Problemy prostsze, liniowo-separowalne | Problemy zÅ‚oÅ¼one, nieliniowe |
| **ÅatwoÅ›Ä‡ interpretacji** | Åatwa (wagi = waÅ¼noÅ›Ä‡ cech) | Trudna (czarna skrzynka) |

**Analogia:**
- Regresja logistyczna = prosta: "JeÅ›li cholesterol > 200 i wiek > 50, to chory"
- MLP = zawiÅ‚e reguÅ‚y: "JeÅ›li ((cholesterol wysoki I mÅ‚ody) LUB (cholesterol Å›redni I stary I ciÅ›nienie...)) TO..."

---

## Podsumowanie - klucze do zrozumienia

### Lab 2 - Regresja logistyczna
ğŸ”‘ **Kluczowa idea:** Pojedynczy neuron, ktÃ³ry uczy siÄ™ prostej granicy decyzyjnej
ğŸ”§ **NarzÄ™dzia:** Sigmoid (przeksztaÅ‚ca na prawdopodobieÅ„stwo), BCE (funkcja kosztu), gradient descent (uczenie)
ğŸ’¡ **Zastosowanie:** Proste problemy klasyfikacji binarnej

### Lab 3 - MLP
ğŸ”‘ **Kluczowa idea:** Stos warstw, ktÃ³re uczÄ… siÄ™ coraz bardziej abstrakcyjnych wzorcÃ³w
ğŸ”§ **NarzÄ™dzia:** ReLU (aktywacja dla ukrytych), backpropagation (uczenie caÅ‚ej sieci), forward/backward pass
ğŸ’¡ **Zastosowanie:** ZÅ‚oÅ¼one problemy, nieliniowe zaleÅ¼noÅ›ci

### NajwaÅ¼niejsze "aha!" momenty

1. **Funkcje aktywacji wprowadzajÄ… nieliniowoÅ›Ä‡** - bez nich caÅ‚a sieÄ‡ = jedna wielka regresja liniowa!

2. **Backpropagation to "gÅ‚uchy telefon" z gradientami** - kaÅ¼da warstwa przekazuje "winÄ™" do poprzedniej

3. **Learning rate kontroluje tempo uczenia** - za maÅ‚y = wolno, za duÅ¼y = chaos

4. **Normalizacja danych jest KRYTYCZNA** - bez niej uczenie bÄ™dzie wolne i niestabilne

5. **ReLU > Sigmoid w warstwach ukrytych** - prostszy, szybszy, bez zanikajÄ…cego gradientu

6. **WiÄ™cej warstw â‰  zawsze lepiej** - potrzebujesz wiÄ™cej danych i czasu na trening

7. **Metryki poza accuracy sÄ… waÅ¼ne** - zwÅ‚aszcza dla niezbalansowanych klas

Powodzenia w laboratoriach! ğŸš€

---

## ğŸ“š BONUS: Cheat sheet - "SkÄ…d siÄ™ biorÄ… wzory?"

### Kluczowe wyprowadzenia w piguÅ‚ce

#### 1ï¸âƒ£ Pochodna Sigmoid: $\sigma'(z) = \sigma(z)(1-\sigma(z))$
**Metoda:** ReguÅ‚a Å‚aÅ„cuchowa na $\frac{1}{1+e^{-z}}$
**Klucz:** Rozbij na $(1+e^{-z})^{-1}$ i rÃ³Å¼niczkuj zÅ‚oÅ¼enie

#### 2ï¸âƒ£ Binary Cross-Entropy: $L = -[y\log(\hat{y}) + (1-y)\log(1-\hat{y})]$
**Å¹rÃ³dÅ‚o:** Maksymalizacja prawdopodobieÅ„stwa (Maximum Likelihood)
**Klucz:** $P(y|x) = \hat{y}^y(1-\hat{y})^{1-y}$ â†’ weÅº $-\log$

#### 3ï¸âƒ£ Gradient dla regresji logistycznej: $\frac{\partial J}{\partial w} = \frac{1}{m}X^T(\hat{y}-y)$
**Metoda:** ReguÅ‚a Å‚aÅ„cuchowa: $\frac{\partial J}{\partial w} = \frac{\partial J}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z} \cdot \frac{\partial z}{\partial w}$
**Klucz:** Pochodne sigmoid i BCE siÄ™ skracajÄ…, zostaje bÅ‚Ä…d Ã— wejÅ›cie!

#### 4ï¸âƒ£ Backpropagation: $\frac{\partial L}{\partial W^{[l]}} = (a^{[l-1]})^T \frac{\partial L}{\partial z^{[l]}}$
**Metoda:** ReguÅ‚a Å‚aÅ„cuchowa + algebra macierzy
**Klucz:** Gradient = "co weszÅ‚o" Ã— "jak siÄ™ pomyliÅ‚o"

#### 5ï¸âƒ£ Gradient dla ostatniej warstwy: $\frac{\partial L}{\partial z^{[last]}} = \hat{y} - y$
**Metoda:** Sigmoid + BCE, pochodne siÄ™ upraszczajÄ…
**Klucz:** $\hat{y}(1-\hat{y})$ z sigmoid skraca siÄ™ z BCE!

#### 6ï¸âƒ£ He initialization: $W \sim \mathcal{N}(0, \sqrt{2/n_{in}})$
**Å¹rÃ³dÅ‚o:** Analiza wariancji aktywacji
**Klucz:** Chcemy $Var(z) \approx 1$, ReLU zeruje poÅ‚owÄ™ â†’ mnoÅ¼nik 2

#### 7ï¸âƒ£ Pochodna ReLU: $\text{ReLU}'(z) = \mathbb{1}_{z>0}$
**Metoda:** Nachylenie prostych odcinkÃ³w
**Klucz:** Dla $z>0$ nachylenie=1, dla $z \leq 0$ nachylenie=0

### Uniwersalna strategia wyprowadzania

**Dla kaÅ¼dego wzoru:**

1. **Zidentyfikuj zÅ‚oÅ¼enie funkcji** 
   - Co zaleÅ¼y od czego? $L(y, \hat{y}(\sigma(z(W, x))))$

2. **ReguÅ‚a Å‚aÅ„cuchowa!**
   - RÃ³Å¼niczkuj od zewnÄ…trz do wewnÄ…trz

3. **Szukaj skrÃ³ceÅ„**
   - CzÄ™sto elementy siÄ™ skracajÄ… (to nie przypadek - te funkcje dobrano wÅ‚aÅ›nie dlatego!)

4. **SprawdÅº wymiary**
   - Gradient musi mieÄ‡ ten sam ksztaÅ‚t co zmienna po ktÃ³rej rÃ³Å¼niczkujesz

5. **Test zdroworozsÄ…dkowy**
   - WiÄ™kszy bÅ‚Ä…d â†’ wiÄ™ksza korekta? âœ“
   - WiÄ™ksze wejÅ›cie â†’ wiÄ™kszy wpÅ‚yw na gradient? âœ“

### NajwaÅ¼niejsze narzÄ™dzia matematyczne

**ReguÅ‚a Å‚aÅ„cuchowa:**
$$\frac{df(g(x))}{dx} = \frac{df}{dg} \cdot \frac{dg}{dx}$$

**Pochodna iloczynu:**
$$\frac{d(fg)}{dx} = f'g + fg'$$

**Pochodna ilorazu:**
$$\frac{d(f/g)}{dx} = \frac{f'g - fg'}{g^2}$$

**Pochodna wykÅ‚adniczej:**
$$\frac{d(e^x)}{dx} = e^x$$

**Pochodna logarytmu:**
$$\frac{d(\ln x)}{dx} = \frac{1}{x}$$

**Algebra macierzy:**
- $(AB)^T = B^T A^T$
- Gradient wzglÄ™dem macierzy czÄ™sto wymaga transpozycji

---

**PamiÄ™taj:** Te wzory NIE spadÅ‚y z nieba! KaÅ¼dy ma logiczne wyprowadzenie. JeÅ›li nie rozumiesz wzoru - wrÃ³Ä‡ do wyprowadzenia krok po kroku! ğŸ“âœ¨
